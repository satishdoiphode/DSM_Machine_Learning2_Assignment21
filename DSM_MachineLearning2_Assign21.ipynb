{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "boston = load_boston()\n",
    "bos = pd.DataFrame(boston.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        0     1     2    3      4      5     6       7    8      9     10  \\\n",
       "0  0.00632  18.0  2.31  0.0  0.538  6.575  65.2  4.0900  1.0  296.0  15.3   \n",
       "1  0.02731   0.0  7.07  0.0  0.469  6.421  78.9  4.9671  2.0  242.0  17.8   \n",
       "2  0.02729   0.0  7.07  0.0  0.469  7.185  61.1  4.9671  2.0  242.0  17.8   \n",
       "3  0.03237   0.0  2.18  0.0  0.458  6.998  45.8  6.0622  3.0  222.0  18.7   \n",
       "4  0.06905   0.0  2.18  0.0  0.458  7.147  54.2  6.0622  3.0  222.0  18.7   \n",
       "\n",
       "       11    12  \n",
       "0  396.90  4.98  \n",
       "1  396.90  9.14  \n",
       "2  392.83  4.03  \n",
       "3  394.63  2.94  \n",
       "4  396.90  5.33  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boston House Prices dataset\n",
      "===========================\n",
      "\n",
      "Notes\n",
      "------\n",
      "Data Set Characteristics:  \n",
      "\n",
      "    :Number of Instances: 506 \n",
      "\n",
      "    :Number of Attributes: 13 numeric/categorical predictive\n",
      "    \n",
      "    :Median Value (attribute 14) is usually the target\n",
      "\n",
      "    :Attribute Information (in order):\n",
      "        - CRIM     per capita crime rate by town\n",
      "        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
      "        - INDUS    proportion of non-retail business acres per town\n",
      "        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
      "        - NOX      nitric oxides concentration (parts per 10 million)\n",
      "        - RM       average number of rooms per dwelling\n",
      "        - AGE      proportion of owner-occupied units built prior to 1940\n",
      "        - DIS      weighted distances to five Boston employment centres\n",
      "        - RAD      index of accessibility to radial highways\n",
      "        - TAX      full-value property-tax rate per $10,000\n",
      "        - PTRATIO  pupil-teacher ratio by town\n",
      "        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
      "        - LSTAT    % lower status of the population\n",
      "        - MEDV     Median value of owner-occupied homes in $1000's\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "    :Creator: Harrison, D. and Rubinfeld, D.L.\n",
      "\n",
      "This is a copy of UCI ML housing dataset.\n",
      "http://archive.ics.uci.edu/ml/datasets/Housing\n",
      "\n",
      "\n",
      "This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\n",
      "\n",
      "The Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\n",
      "prices and the demand for clean air', J. Environ. Economics & Management,\n",
      "vol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\n",
      "...', Wiley, 1980.   N.B. Various transformations are used in the table on\n",
      "pages 244-261 of the latter.\n",
      "\n",
      "The Boston house-price data has been used in many machine learning papers that address regression\n",
      "problems.   \n",
      "     \n",
      "**References**\n",
      "\n",
      "   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\n",
      "   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\n",
      "   - many more! (see http://archive.ics.uci.edu/ml/datasets/Housing)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print (boston.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we’ll load the data to Pandas\n",
    "# define the data/predictors as the pre-set feature names  \n",
    "df = pd.DataFrame(boston.data, columns=boston.feature_names)\n",
    "\n",
    "# Put the target (housing value -- MEDV) in another DataFrame\n",
    "target = pd.DataFrame(boston.target, columns=[\"MEDV\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "##So now, as before, we have the data frame that contains the independent variables (marked as “df”) \n",
    "#and the data frame with the dependent variable (marked as “target”). Let’s fit a regression model using SKLearn. \n",
    "#First we’ll define our X and y — this time I’ll use all the variables in the data frame to predict the \n",
    "#housing price:\n",
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "X = df\n",
    "y = target [\"MEDV\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "lm = linear_model.LinearRegression()\n",
    "model = lm.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "##The lm.fit() function fits a linear model. We want to use the model to make predictions \n",
    "#(that’s what we’re here for!), so we’ll use lm.predict():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[30.00821269 25.0298606  30.5702317  28.60814055 27.94288232 25.25940048\n",
      " 23.00433994 19.5347558  11.51696539 18.91981483 18.9958266  21.58970854\n",
      " 20.90534851 19.55535931 19.2837957  19.30000174 20.52889993 16.9096749\n",
      " 16.17067411 18.40781636 12.52040454 17.67104565 15.82934891 13.80368317\n",
      " 15.67708138 13.3791645  15.46258829 14.69863607 19.54518512 20.87309945\n",
      " 11.44806825 18.05900412  8.78841666 14.27882319 13.69097132 23.81755469\n",
      " 22.34216285 23.11123204 22.91494157 31.35826216 34.21485385 28.0207132\n",
      " 25.20646572 24.61192851 22.94438953 22.10150945 20.42467417 18.03614022\n",
      "  9.10176198 17.20856571 21.28259372 23.97621248 27.65853521 24.0521088\n",
      " 15.35989132 31.14817003 24.85878746 33.11017111 21.77458036 21.08526739\n",
      " 17.87203538 18.50881381 23.9879809  22.54944098 23.37068403 30.36557584\n",
      " 25.53407332 21.11758504 17.42468223 20.7893086  25.20349174 21.74490595\n",
      " 24.56275612 24.04479519 25.5091157  23.97076758 22.94823519 23.36106095\n",
      " 21.26432549 22.4345376  28.40699937 26.99734716 26.03807246 25.06152125\n",
      " 24.7858613  27.79291889 22.16927073 25.89685664 30.67771522 30.83225886\n",
      " 27.12127354 27.41597825 28.9456478  29.08668003 27.04501726 28.62506705\n",
      " 24.73038218 35.78062378 35.11269515 32.25115468 24.57946786 25.59386215\n",
      " 19.76439137 20.31157117 21.4353635  18.53971968 17.18572611 20.74934949\n",
      " 22.64791346 19.77000977 20.64745349 26.52652691 20.77440554 20.71546432\n",
      " 25.17461484 20.4273652  23.37862521 23.69454145 20.33202239 20.79378139\n",
      " 21.92024414 22.47432006 20.55884635 16.36300764 20.56342111 22.48570454\n",
      " 14.61264839 15.1802607  18.93828443 14.0574955  20.03651959 19.41306288\n",
      " 20.06401034 15.76005772 13.24771577 17.26167729 15.87759672 19.36145104\n",
      " 13.81270814 16.44782934 13.56511101  3.98343974 14.59241207 12.14503093\n",
      "  8.72407108 12.00815659 15.80308586  8.50963929  9.70965512 14.79848067\n",
      " 20.83598096 18.30017013 20.12575267 17.27585681 22.35997992 20.07985184\n",
      " 13.59903744 33.26635221 29.03938379 25.56694529 32.71732164 36.78111388\n",
      " 40.56615533 41.85122271 24.79875684 25.3771545  37.20662185 23.08244608\n",
      " 26.40326834 26.65647433 22.55412919 24.2970948  22.98024802 29.07488389\n",
      " 26.52620066 30.72351225 25.61835359 29.14203283 31.43690634 32.9232938\n",
      " 34.72096487 27.76792733 33.88992899 30.99725805 22.72124288 24.76567683\n",
      " 35.88131719 33.42696242 32.41513625 34.51611818 30.76057666 30.29169893\n",
      " 32.92040221 32.11459912 31.56133385 40.84274603 36.13046343 32.66639271\n",
      " 34.70558647 30.09276228 30.64139724 29.29189704 37.07062623 42.02879611\n",
      " 43.18582722 22.6923888  23.68420569 17.85435295 23.49543857 17.00872418\n",
      " 22.39535066 17.06152243 22.74106824 25.21974252 11.10601161 24.51300617\n",
      " 26.60749026 28.35802444 24.91860458 29.69254951 33.18492755 23.77145523\n",
      " 32.14086508 29.74802362 38.36605632 39.80716458 37.58362546 32.39769704\n",
      " 35.45048257 31.23446481 24.48478321 33.28615723 38.04368164 37.15737267\n",
      " 31.71297469 25.26658017 30.101515   32.71897655 28.42735376 28.42999168\n",
      " 27.2913215  23.74446671 24.11878941 27.40241209 16.32993575 13.39695213\n",
      " 20.01655581 19.86205904 21.28604604 24.07796482 24.20603792 25.04201534\n",
      " 24.91709097 29.93762975 23.97709054 21.69931969 37.51051381 43.29459357\n",
      " 36.48121427 34.99129701 34.80865729 37.16296374 40.9823638  34.44211691\n",
      " 35.83178068 28.24913647 31.22022312 40.83256202 39.31768808 25.71099424\n",
      " 22.30344878 27.20551341 28.51386352 35.47494122 36.11110647 33.80004807\n",
      " 35.61141951 34.84311742 30.35359323 35.31260262 38.79684808 34.33296541\n",
      " 40.34038636 44.67339923 31.5955473  27.35994642 20.09520596 27.04518524\n",
      " 27.21674397 26.91105226 33.43602979 34.40228785 31.83374181 25.82416035\n",
      " 24.43687139 28.46348891 27.36916176 19.54441878 29.11480679 31.90852699\n",
      " 30.77325183 28.9430835  28.88108106 32.79876794 33.20356949 30.76568546\n",
      " 35.55843485 32.70725436 28.64759861 23.59388439 18.5461558  26.88429024\n",
      " 23.28485442 25.55002201 25.48337323 20.54343769 17.61406384 18.37627933\n",
      " 24.29187594 21.3257202  24.88826131 24.87143049 22.87255605 19.4540234\n",
      " 25.11948741 24.66816374 23.68209656 19.33951725 21.17636041 24.25306588\n",
      " 21.59311197 19.98766667 23.34079584 22.13973959 21.55349196 20.61808868\n",
      " 20.1607571  19.28455466 22.16593919 21.24893735 21.42985456 30.32874523\n",
      " 22.04915396 27.70610125 28.54595004 16.54657063 14.78278261 25.27336772\n",
      " 27.54088054 22.14633467 20.46081206 20.54472332 16.88194391 25.40066956\n",
      " 14.32299547 16.5927403  19.63224597 22.7117302  22.19946949 19.1989151\n",
      " 22.66091019 18.92059374 18.22715359 20.22444386 37.47946099 14.29172583\n",
      " 15.53697148 10.82825817 23.81134987 32.64787163 34.61163401 24.94604102\n",
      " 26.00259724  6.12085728  0.78021126 25.311373   17.73465914 20.22593282\n",
      " 15.83834861 16.83742401 14.43123608 18.47647773 13.42427933 13.05677824\n",
      "  3.27646485  8.05936467  6.13903114  5.62271213  6.44935154 14.20597451\n",
      " 17.21022671 17.29035065  9.89064351 20.21972222 17.94511052 20.30017588\n",
      " 19.28790318 16.33300008  6.56843662 10.87541577 11.88704097 17.81098929\n",
      " 18.25461066 12.99282707  7.39319053  8.25609561  8.07899971 19.98563715\n",
      " 13.69651744 19.83511412 15.2345378  16.93112419  1.69347406 11.81116263\n",
      " -4.28300934  9.55007844 13.32635521  6.88351077  6.16827417 14.56933235\n",
      " 19.59292932 18.1151686  18.52011987 13.13707457 14.59662601  9.8923749\n",
      " 16.31998048 14.06750301 14.22573568 13.00752251 18.13277547 18.66645496\n",
      " 21.50283795 17.00039379 15.93926602 13.32952716 14.48949211  8.78366731\n",
      "  4.8300317  13.06115528 12.71101472 17.2887624  18.73424906 18.05271013\n",
      " 11.49855612 13.00841512 17.66975577 18.12342294 17.51503231 17.21307203\n",
      " 16.48238543 19.40079737 18.57392951 22.47833186 15.24179836 15.78327609\n",
      " 12.64853778 12.84121049 17.17173661 18.50906858 19.02803874 20.16441773\n",
      " 19.76975335 22.42614937 20.31750314 17.87618837 14.3391341  16.93715603\n",
      " 16.98716629 18.59431701 20.16395155 22.97743546 22.45110639 25.5707207\n",
      " 16.39091112 16.09765427 20.52835689 11.5429045  19.20387482 21.86820603\n",
      " 23.47052203 27.10034494 28.57064813 21.0839881  19.4490529  22.2189221\n",
      " 19.65423066 21.324671   11.86231364  8.22260592  3.65825168 13.76275951\n",
      " 15.93780944 20.62730097 20.61035443 16.88048035 14.01017244 19.10825534\n",
      " 21.29720741 18.45524217 20.46764235 23.53261729 22.37869798 27.62934247\n",
      " 26.12983844 22.34870269]\n"
     ]
    }
   ],
   "source": [
    "##The print function would print the first 5 predictions for y (I didn’t print the entire list to “save room”. \n",
    "#Removing [0:5] would print \n",
    "#the entire list):\n",
    "predictions = lm.predict(X)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lm.predict() predicts the y (dependent variable) using the linear model we fitted. \n",
    "#You must have noticed that when we run a linear regression with SKLearn, we don’t get a pretty table \n",
    "#(okay, it’s not that pretty… but it’s pretty useful) like in Statsmodels. What we can do is use built-in functions \n",
    "#to return the score, the coefficients and the estimated intercepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7406077428649428"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.score(X,y) #This is the R² score of our model. As you probably remember, this the percentage of explained \n",
    "#variance of the predictions. If you’re interested, read more here. Next, let’s check out the coefficients \n",
    "#for the predictors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.07170557e-01,  4.63952195e-02,  2.08602395e-02,  2.68856140e+00,\n",
       "       -1.77957587e+01,  3.80475246e+00,  7.51061703e-04, -1.47575880e+00,\n",
       "        3.05655038e-01, -1.23293463e-02, -9.53463555e-01,  9.39251272e-03,\n",
       "       -5.25466633e-01])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.coef_ # will give coeficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36.4911032803614"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.intercept_ # will give intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#These are all (estimated/predicted) parts of the multiple regression equation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
